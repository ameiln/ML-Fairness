{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"text-align:center;\"> <span style=\"color:green; font-weight:bold; font-size:24px;\"> Pre-Processing Algorithms in ML Fairness: Disparate Impact Remover </span> </p>","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:38:12.983921Z","iopub.execute_input":"2023-08-15T19:38:12.984355Z","iopub.status.idle":"2023-08-15T19:38:13.012886Z","shell.execute_reply.started":"2023-08-15T19:38:12.984322Z","shell.execute_reply":"2023-08-15T19:38:13.010962Z"}}},{"cell_type":"markdown","source":"**Introduction**\n\nIn the realm of machine learning, ensuring fairness is a paramount concern to avoid discrimination and bias in model predictions. This post aims to provide an insightful guide on utilizing the Disparate Impact Remover (DIR) method, a powerful preprocessing technique from IBM AIF360 toolkit, to address and mitigate disparities in machine learning models.\n\n**Pre-processing algorithms**\n\nOne effective approach to achieve fairness is through preprocessing techniques. These methods modify the training data before model training, rectifying any underlying biases present in the dataset. By tackling biases at the data level, we lay a solid foundation for equitable model predictions. \n\n**Disparate Impact Remover**\n\nThe Disparate Impact Remover (DIR) is a preprocessing technique provided by AIF360 toolkit. Its primary goal is to mitigate disparate impact, which occurs when different groups are treated unfairly by a model's predictions. DIR works by adjusting the dataset to ensure similar prediction distributions across different sensitive attribute groups.\n\nIn plain English, the disparate impact ratio is the ratio of positive outcomes (for example, Loan_Status=1 or Loan_default=0) in the unprivileged group divided by the ratio of positive outcomes in the privileged group. The AIF360 tool suggests that an acceptable lower bound is 80%. That is, ifÂ the unprivileged group receives a positive outcome less than 80% of the time that the privileged group does, it is a disparate impact violation.\n\n**Problem Statement**\n\nWe use a credit loan decision dataset to evaluate the disparity between genders before and after applying the DIR: \n* First, we evaluate the disparity between the positive outcomes in different genders in the original dataset.\n* Then, we tranform the dataset using the DIR method.\n* Next, we train a LightGBM model using the transformed dataset and predict the test datset\n* Finally, we compare the outcomes of the ML model with the original dataset.","metadata":{}},{"cell_type":"markdown","source":"## **1. Install Required Packages - Import Libraries**","metadata":{}},{"cell_type":"code","source":"pip install aif360","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install xlrd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install BlackBoxAuditing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom aif360.algorithms.preprocessing import DisparateImpactRemover\nfrom aif360.datasets import BinaryLabelDataset, Dataset\nfrom aif360.metrics import BinaryLabelDatasetMetric\n\nrand_seed = 1234\nnp.random.seed(rand_seed)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T18:54:27.105821Z","iopub.execute_input":"2023-08-15T18:54:27.106278Z","iopub.status.idle":"2023-08-15T18:54:27.115115Z","shell.execute_reply.started":"2023-08-15T18:54:27.106245Z","shell.execute_reply":"2023-08-15T18:54:27.113694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2. Dataset**","metadata":{}},{"cell_type":"markdown","source":"The dataset we'll use for this tutorial is the credit loan decision used in the Ernst & Young case study published on Microsoft Fairlearn. By utilizing DIR, we'll showcase how to reduce disparities in the dataset and align model predictions with fairness goals.\n \nTo download the dataset or to use it in your analysis, you can refer to [this link](https://www.kaggle.com/datasets/mohammadbolandraftar/credit-card-defaults-in-taiwan-ml-fairness), and for more information about the case study, please refer to [Fairlearn page](https://fairlearn.org/main/auto_examples/plot_credit_loan_decisions.html).\n\n**Note:** To work with the DIR, we need to define the privileged/unprivileged groups and favorable/unfavorable outcomes. In our case, default=1 is unfavorable and default=0 is favorable. We also assume that female applicants are the unprivileged group.","metadata":{}},{"cell_type":"code","source":"data_url = \"/kaggle/input/credit-card-defaults-in-taiwan-ml-fairness/default of credit card clients (1).xls\"\ndataset = pd.read_excel(data_url, header=1).drop(columns=[\"ID\"]).rename(columns={\"PAY_0\": \"PAY_1\", \"default payment next month\": \"default\"})\n\nprint(\"size of dataset:\", dataset.shape)\nprint()\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T18:55:46.470751Z","iopub.execute_input":"2023-08-15T18:55:46.472277Z","iopub.status.idle":"2023-08-15T18:55:50.782140Z","shell.execute_reply.started":"2023-08-15T18:55:46.472227Z","shell.execute_reply":"2023-08-15T18:55:50.780706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2.1. Evaluate Disparity Between Genders**","metadata":{}},{"cell_type":"markdown","source":"There are two ways to evaluate disparities:\n\n1. Calculating the disparity proportion through Python coding\n2. We can also use the AIF360 metrics to calculate the disparity ratio\n\nI've provided the both methods in the following cells.","metadata":{}},{"cell_type":"markdown","source":"**2.1.1. Calculate Manually**","metadata":{}},{"cell_type":"code","source":"def calc_prop(data, group_col, group, output_col, output_val):\n    new = data[data[group_col] == group]\n    return len(new[new[output_col] == output_val])/len(new)\n\nparity_rate_male = calc_prop(dataset, \"SEX\", 1, \"default\", 0)\nparity_rate_female = calc_prop(dataset, \"SEX\", 2, \"default\", 0)\nprint(\"parity_rate_male:\", parity_rate_male)\nprint(\"parity_rate_female:\", parity_rate_female)\nprint(\"parity rate ration female/male:\", parity_rate_female/parity_rate_male)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T18:57:45.519900Z","iopub.execute_input":"2023-08-15T18:57:45.520333Z","iopub.status.idle":"2023-08-15T18:57:45.540337Z","shell.execute_reply.started":"2023-08-15T18:57:45.520303Z","shell.execute_reply":"2023-08-15T18:57:45.538918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.1.2. Use AIF360 Metrics**","metadata":{}},{"cell_type":"markdown","source":"First, we need the define privileged and unprivileged groups","metadata":{}},{"cell_type":"code","source":"privileged_group = [{'SEX': 1}]\nunprivileged_group = [{'SEX': 2}]","metadata":{"execution":{"iopub.status.busy":"2023-08-15T18:57:55.197002Z","iopub.execute_input":"2023-08-15T18:57:55.197487Z","iopub.status.idle":"2023-08-15T18:57:55.203666Z","shell.execute_reply.started":"2023-08-15T18:57:55.197444Z","shell.execute_reply":"2023-08-15T18:57:55.202061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we need to convert DataFrame to BinaryLabelDataset:","metadata":{}},{"cell_type":"code","source":"binary_label_dataset_original = BinaryLabelDataset(df=dataset, \n                                        label_names=['default'], \n                                        protected_attribute_names=['SEX'],\n                                        favorable_label=0, # non-default\n                                        unfavorable_label=1, # default label\n                                        )","metadata":{"execution":{"iopub.status.busy":"2023-08-15T18:58:32.445184Z","iopub.execute_input":"2023-08-15T18:58:32.445659Z","iopub.status.idle":"2023-08-15T18:58:32.555686Z","shell.execute_reply.started":"2023-08-15T18:58:32.445620Z","shell.execute_reply":"2023-08-15T18:58:32.554330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this stage, we can calculate the Disparate Impact:","metadata":{}},{"cell_type":"code","source":"di = BinaryLabelDatasetMetric(binary_label_dataset_original,\n                              unprivileged_groups = unprivileged_group,\n                              privileged_groups = privileged_group\n                             ).disparate_impact()\n\nprint(\"Disparate Impact:\", di)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T18:59:16.774128Z","iopub.execute_input":"2023-08-15T18:59:16.774565Z","iopub.status.idle":"2023-08-15T18:59:16.783837Z","shell.execute_reply.started":"2023-08-15T18:59:16.774533Z","shell.execute_reply":"2023-08-15T18:59:16.782598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2.2. Change Dataset Distribution**\n\nAs we see, there is not a large disparity between the positive outcomes in male and female applicants. So, **for educational purposes**, we change the distribution of positive outcomes in females such that they receive less positive outcome. This helps us define them as our unprivileged group. The following code changes 40% of default values from 0 to 1 in females applicants.","metadata":{}},{"cell_type":"code","source":"df = dataset.copy()\n\n# filters the non-default females\nmask = (df['SEX'] == 2) & (df['default'] == 0)\n\n# picks 40% of the filtered rows randomly \nrows_to_change = df[mask].sample(frac=0.4, random_state=42)\n\n# Change the default values to 1 for the selected rows\ndf.loc[rows_to_change.index, 'default'] = 1","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:15:42.548363Z","iopub.execute_input":"2023-08-15T19:15:42.548854Z","iopub.status.idle":"2023-08-15T19:15:42.565640Z","shell.execute_reply.started":"2023-08-15T19:15:42.548815Z","shell.execute_reply":"2023-08-15T19:15:42.564675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_features = [col for col in df.columns if col not in [\"default\"]]\nmodel_target = [\"default\"]","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:15:45.701267Z","iopub.execute_input":"2023-08-15T19:15:45.701676Z","iopub.status.idle":"2023-08-15T19:15:45.707568Z","shell.execute_reply.started":"2023-08-15T19:15:45.701645Z","shell.execute_reply":"2023-08-15T19:15:45.706305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2.3. Evaluate DIR After Changing Dataset Distribution**","metadata":{}},{"cell_type":"markdown","source":"let's evaluate the disparity impact after changing the dataset distribution","metadata":{}},{"cell_type":"code","source":"binary_label_dataset_new = BinaryLabelDataset(df=df, # from now on, we'll work on the df\n                                              label_names=['default'], \n                                              protected_attribute_names=['SEX'],\n                                              favorable_label=0, # non-default\n                                              unfavorable_label=1, # default label\n                                              )\n\n# Calculate disparate impact in the new dataset\ndi_new= BinaryLabelDatasetMetric(binary_label_dataset_new,\n                                 unprivileged_groups = unprivileged_group,\n                                 privileged_groups = privileged_group\n                                ).disparate_impact()\n\nprint(\"Disparate Impact in the new dataset:\", di_new)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:15:48.562221Z","iopub.execute_input":"2023-08-15T19:15:48.562716Z","iopub.status.idle":"2023-08-15T19:15:48.677306Z","shell.execute_reply.started":"2023-08-15T19:15:48.562674Z","shell.execute_reply":"2023-08-15T19:15:48.676063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code worked. Now, the disparity ratio is around 63% which is far lower than 80% for the lower band of parity.","metadata":{}},{"cell_type":"markdown","source":"## **3. Transform Dataset by Applying Disparate Impact Remover**","metadata":{}},{"cell_type":"markdown","source":"In the first step, we need to apply the DIR and transform the dataset. Then, we use the \"convert_to_dataframe\" method to create a dataframe of the transformed dataset. \nThe DIR algorithm requires the user to specify a repair_level, this indicates how much you wish for the distributions of the groups to overlap. Repair_level takes values between 0.0 and 1.0. Repair amount 0.0 means no repair while 1.0 is full repair.","metadata":{}},{"cell_type":"code","source":"def apply_dir(rep_lev, bin_label_data):\n    di_remover = DisparateImpactRemover(repair_level=rep_lev)\n    binary_label_dataset_transformed = di_remover.fit_transform(bin_label_data)\n    df_transformed = binary_label_dataset_transformed.convert_to_dataframe()[0] # create the transformed dataframe\n    return df_transformed","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:15:51.045116Z","iopub.execute_input":"2023-08-15T19:15:51.045577Z","iopub.status.idle":"2023-08-15T19:15:51.052886Z","shell.execute_reply.started":"2023-08-15T19:15:51.045541Z","shell.execute_reply":"2023-08-15T19:15:51.051430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this step, I'm applying DIR on the \"df\" to show you how the dataset is transformed. The name of the transformed dataset is \"transformed_df\". I'm using the repair_level of 0.9. ","metadata":{}},{"cell_type":"code","source":"# Convert DataFrame to BinaryLabelDataset\nbinary_label_dataset = BinaryLabelDataset(df=df, # Our goal is to transform the df dataset\n                                        label_names=['default'], \n                                        protected_attribute_names=['SEX'],\n                                        favorable_label=0, # non-default\n                                        unfavorable_label=1, # default label\n                                        )\n\ndf_transformed = apply_dir(0.9, binary_label_dataset) # repair_level=0.9","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:15:53.133023Z","iopub.execute_input":"2023-08-15T19:15:53.133446Z","iopub.status.idle":"2023-08-15T19:16:00.474427Z","shell.execute_reply.started":"2023-08-15T19:15:53.133417Z","shell.execute_reply":"2023-08-15T19:16:00.473177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After applying the DisparateImpactRemover, we have transformed the df to df_transformed. Let's check the results:","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:16:00.477507Z","iopub.execute_input":"2023-08-15T19:16:00.478368Z","iopub.status.idle":"2023-08-15T19:16:00.500313Z","shell.execute_reply.started":"2023-08-15T19:16:00.478322Z","shell.execute_reply":"2023-08-15T19:16:00.498978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_transformed.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:16:03.645158Z","iopub.execute_input":"2023-08-15T19:16:03.645573Z","iopub.status.idle":"2023-08-15T19:16:03.682425Z","shell.execute_reply.started":"2023-08-15T19:16:03.645543Z","shell.execute_reply":"2023-08-15T19:16:03.681198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, some elements, e.g. rows 2 and 3 in the PAY_AMT1 have changed.","metadata":{}},{"cell_type":"markdown","source":"**Visualize the distribution of data in \"BILL_AMT6\" before and after applying DIR**","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\n# Initialize figure\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Set title of figure\nfig.suptitle(\"Comparison of Feature before and after Transformation\")\n\n# Set title\nax1.title.set_text(\"Original distribution\")\nax2.title.set_text(\"Transformed distribution\")\n\n# Create plots\nsns.histplot(df, x=\"BILL_AMT6\", hue=\"SEX\", bins=50, ax=ax1)\nsns.histplot(df_transformed, x=\"BILL_AMT6\", hue=\"SEX\", bins=50, ax=ax2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:16:06.263961Z","iopub.execute_input":"2023-08-15T19:16:06.264671Z","iopub.status.idle":"2023-08-15T19:16:07.426031Z","shell.execute_reply.started":"2023-08-15T19:16:06.264636Z","shell.execute_reply":"2023-08-15T19:16:07.424698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color:red\"> **Important Note:** By calculating the disparity impact in the transformed dataset, you'll notice that it's equal to the original dataset. This is due to the fact that you changed the distribution of data in dataset features except in the protected feature (\"SEX\" in our case) and target feature (\"default\"). So, there will be no change in the Disparity Impact. Let's check it together:</span> ","metadata":{}},{"cell_type":"code","source":"parity_rate_male = calc_prop(df_transformed, \"SEX\", 1, \"default\", 0)\nparity_rate_female = calc_prop(df_transformed, \"SEX\", 2, \"default\", 0)\nprint(\"parity_rate_male:\", parity_rate_male)\nprint(\"parity_rate_female:\", parity_rate_female)\nprint(\"parity rate ration female/male:\", parity_rate_female/parity_rate_male)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:17:36.183816Z","iopub.execute_input":"2023-08-15T19:17:36.184275Z","iopub.status.idle":"2023-08-15T19:17:36.200304Z","shell.execute_reply.started":"2023-08-15T19:17:36.184242Z","shell.execute_reply":"2023-08-15T19:17:36.199084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see, the disparity rate hasn't changed.","metadata":{}},{"cell_type":"markdown","source":"## **4. Create Train & Test Datasets**","metadata":{}},{"cell_type":"code","source":"def tr_test_splt(df_tr, features, target, df):\n    train_df_transformed, test_df_transformed = train_test_split(df_tr, \n                                                                 test_size=0.3, \n                                                                 random_state=42, \n                                                                 shuffle=True)\n    \n    train_df, test_df = train_test_split(df, \n                                         test_size=0.3, \n                                         random_state=42, \n                                         shuffle=True)\n\n    x_train_transformed = train_df_transformed[features]\n    y_train_transformed = train_df_transformed[target]\n\n    x_test_transformed = test_df_transformed[features]\n    y_test_transformed = test_df_transformed[target]\n    \n    return test_df, train_df_transformed, test_df_transformed, x_train_transformed, y_train_transformed, x_test_transformed, y_test_transformed","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:17:49.037350Z","iopub.execute_input":"2023-08-15T19:17:49.037798Z","iopub.status.idle":"2023-08-15T19:17:49.047035Z","shell.execute_reply.started":"2023-08-15T19:17:49.037759Z","shell.execute_reply":"2023-08-15T19:17:49.045550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **5. Create the ML model**","metadata":{}},{"cell_type":"code","source":"def lgb_model(x_train_transformed, y_train_transformed, x_test_transformed):\n    # Initialize LightGBM model\n    lgb_params = {\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"learning_rate\": 0.03,\n        \"num_leaves\": 10,\n        \"max_depth\": 3,\n        \"random_state\": rand_seed,\n        \"n_jobs\": 1,\n    }\n\n    lgb_dir = Pipeline(\n        steps=[\n            (\"preprocessing\", StandardScaler()),\n            (\"classifier\", lgb.LGBMClassifier(**lgb_params)), \n              ]\n                        )\n\n    lgb_dir.fit(x_train_transformed, y_train_transformed.values.ravel())\n\n    y_pred_test_lgb_dir = lgb_dir.predict(x_test_transformed)\n    \n    return y_pred_test_lgb_dir","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:18:17.720138Z","iopub.execute_input":"2023-08-15T19:18:17.720594Z","iopub.status.idle":"2023-08-15T19:18:17.730993Z","shell.execute_reply.started":"2023-08-15T19:18:17.720561Z","shell.execute_reply":"2023-08-15T19:18:17.729525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **6. Build DataFrame for Evaluation purposes**","metadata":{}},{"cell_type":"markdown","source":"In this section we evaluate the Disparity Impact before and after applying the LightGBM model. To do this, we create a dataframe with the following features: \"SEX\" & \"default\" in the transformed test dataset (before appying the ML model), and the model prediction.","metadata":{}},{"cell_type":"code","source":"def get_results(test_df_transformed, y_pred_test_lgb_dir):\n    all_results = pd.concat(\n        [\n            test_df_transformed.reset_index(drop=True)[[\"SEX\", \"default\"]],\n            pd.Series(y_pred_test_lgb_dir, name=\"y_pred_test_lgb_dir\"),  \n        ],\n        axis=1\n    )\n    return all_results","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:20:14.572710Z","iopub.execute_input":"2023-08-15T19:20:14.573220Z","iopub.status.idle":"2023-08-15T19:20:14.580619Z","shell.execute_reply.started":"2023-08-15T19:20:14.573182Z","shell.execute_reply":"2023-08-15T19:20:14.579250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We create a function to compare the Disparity Impact in the transformed dataset and the model prediction.","metadata":{}},{"cell_type":"code","source":"def gen_dir_results(test_df, all_results):\n\n    # Convert test_df DataFrame to BinaryLabelDatase\n    binary_label_dataset_original_test = BinaryLabelDataset(df=test_df, \n                                                            label_names=['default'], \n                                                            protected_attribute_names=['SEX'],\n                                                            favorable_label=0, \n                                                            unfavorable_label=1, \n                                                            )\n\n    # Calculate disparate impact in the original test dataset, before applying DIR\n    dir_metrics_original_test = BinaryLabelDatasetMetric(binary_label_dataset_original_test,\n                                                        unprivileged_groups = unprivileged_group,\n                                                        privileged_groups = privileged_group\n                                                        ).disparate_impact()\n\n    print(\"Disparate Impact in _original_test:\", dir_metrics_original_test)\n\n    # Convert y_pred_test_lgb_dir (the model prediction) DataFrame to BinaryLabelDataset\n    binary_label_dataset_lgb_dir = BinaryLabelDataset(df=all_results, \n                                                      label_names=['y_pred_test_lgb_dir'], \n                                                      protected_attribute_names=['SEX'],\n                                                      favorable_label=0, \n                                                      unfavorable_label=1, \n                                                      )\n\n    # Calculate disparate impact in the predicted dataset generated by the LightGBM\n    dir_metrics_lgb_dir = BinaryLabelDatasetMetric(binary_label_dataset_lgb_dir,\n                                                   unprivileged_groups = unprivileged_group,\n                                                   privileged_groups = privileged_group\n                                                   ).disparate_impact()\n\n    print(\"Disparate Impact in lgb_dir:\", dir_metrics_lgb_dir)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:22:03.097038Z","iopub.execute_input":"2023-08-15T19:22:03.098277Z","iopub.status.idle":"2023-08-15T19:22:03.112907Z","shell.execute_reply.started":"2023-08-15T19:22:03.098218Z","shell.execute_reply":"2023-08-15T19:22:03.111203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **7. Compare Results**","metadata":{}},{"cell_type":"markdown","source":"In this section, we compare the results before and after applying the LightGBM model. We can change the code such that it transforms the original dataset with different repair levels.","metadata":{}},{"cell_type":"code","source":"for i in range(1, 11):\n    repair_level = i / 10 \n    print(\"repair_level:\", repair_level)\n    df_transformed = apply_dir(repair_level, binary_label_dataset)\n    test_df, train_df_transformed, test_df_transformed, x_train_transformed, y_train_transformed, x_test_transformed, y_test_transformed = tr_test_splt(df_transformed, model_features, model_target, df)\n    y_pred_test_lgb_dir = lgb_model(x_train_transformed, y_train_transformed, x_test_transformed)\n    all_results = get_results(test_df_transformed, y_pred_test_lgb_dir)\n    gen_dir_results(test_df, all_results)\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:22:29.691001Z","iopub.execute_input":"2023-08-15T19:22:29.691470Z","iopub.status.idle":"2023-08-15T19:23:54.117918Z","shell.execute_reply.started":"2023-08-15T19:22:29.691435Z","shell.execute_reply":"2023-08-15T19:23:54.116922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the model prediction has a higher disparity impact for different repair_level values although it's still below the 80% disparity.","metadata":{}},{"cell_type":"markdown","source":"## **8. Conclusion**","metadata":{}},{"cell_type":"markdown","source":"In this example we applied the DIR method to mitigate the disparity between the female and male applicants. We transformed the dataset and then applied LightGBM model. The LightGBM model results using the transformed dataset shows a lower disparity between females and males.\n\n**Considerations**\n\n* As data scientists, it's crucial to acknowledge that the Disparate Impact Remover stands as a viable pre-processing tool to rectify dataset disparities. However, its applicability isn't universal across all datasets. To gauge its effectiveness, a prudent approach involves assessing the original dataset's disparity, followed by comparison with a fairness-unaware model (e.g. applying the LightGBM model on the original dataset in our case) and fairness-aware (applying the LightGBM model on the transformed dataset in this example). Only through this comparison can we ascertain the extent of its effectiveness in transforming the dataset's dynamics.\n* This method may not be applicable on all datasets and that's why we have other pre-processing techniques in fairness libraries.\n* If the pre-processing techniques don't work, we need to test the in-processing or post-processing methods.","metadata":{}}]}